## data preparation: downloads data in FASTQ format using sra toolkit
#working on base environment of conda
mkdir main_reads && cd main_reads 
conda install -y sra-tools
fastq-dump --split-files -X 5000000 SRR8797509
#splitting of main reads into 5 sub-groups
conda install -c bioconda seqkit
seqkit split2 -1 SRR8797509_1.fastq -2 SRR8797509_2.fastq -p 5 -O out -f
#renaming splitted files
sudo apt install rename
rename 's/SRR8797509/main_SRR8797509/' *.fastq

#shuffling main reads & spliting of shuffled groups #this is the proposed code but we download it from slack
mkdir Shuffled_reads && cd Shuffled_reads
time seqkit shuffle -2 -s main_reads/SRR8797509_1.fastq>shuff_SRR8797509_1.part_001.fastq
time seqkit shuffle -2 -s main_reads/SRR8797509_2.fastq>shuff_SRR8797509_2.part_001.fastq
seqkit split2 -1 shuff_SRR8797509_1.part_001.fastq -2 shuff_SRR8797509_2.part_001.fastq -p 5 -O out -f

## FASTQ Quality Control for: main reads
conda install -c bioconda fastqc
conda install -c bioconda multiqc  
mkdir ~/workdir/assignment/FASTQC_tut && cd ~/workdir/assignment/FASTQC_tut
for f in ~/workdir/assignment/ngs1_project/main_reads/out/main_SRR8797509_*.part_001.fastq;do
fastqc -t 1 -f fastq -noextract $f;done
multiqc -z -o . .
#FASTQ Quality Control for shuffled reads 
mkdir ~/workdir/assignment/FASTQC_tut2 && cd ~/workdir/assignment/FASTQC_tut2
for f in ~/workdir/assignment/ngs1_project/Shuffled_reads/out/shuff_SRR8797509_*.part_001.fastq;do
fastqc -t 1 -f fastq -noextract $f;done
multiqc -z -o . .

##Trimming
#mild
conda install -c bioconda trimmomatic 
mkdir ~/workdir/assignment/mild_trimmed && cd ~/workdir/assignment/mild_trimmed
for SAMPLE in 1 2 3 4 5;
    do
        R1=$HOME/workdir/assignment/ngs1_project/main_reads/out/main_SRR8797509_1.part_00${SAMPLE}.fastq
        R2=$HOME/workdir/assignment/ngs1_project/main_reads/out/main_SRR8797509_2.part_00${SAMPLE}.fastq
        newf1=$HOME/workdir/assignment/mild_trimmed/main_SRR8797509_1.part_00${SAMPLE}.pe.trim.fastq
        newf2=$HOME/workdir/assignment/mild_trimmed/main_SRR8797509_2.part_00${SAMPLE}.pe.trim.fastq
        newf1U=$HOME/workdir/assignment/mild_trimmed/main_SRR8797509_1.part_00${SAMPLE}.se.trim.fastq
        newf2U=$HOME/workdir/assignment/mild_trimmed/main_SRR8797509_2.part_00${SAMPLE}.se.trim.fastq

        adap="/home/emmyneutron/miniconda3/envs/ngs1/share/trimmomatic-0.38-1/adapters"

        trimmomatic PE -threads 1 -phred33 -trimlog trimLogFile -summary statsSummaryFile  $R1 $R2 $newf1 $newf1U $newf2 $newf2U \
        ILLUMINACLIP:$adap/TruSeq3-PE.fa:2:30:10:1 SLIDINGWINDOW:4:15 MINLEN:36 
    done
#aggressive
    mkdir ~/workdir/assignment/aggr_trimmed && cd ~/workdir/assignment/aggr_trimmed
for SAMPLE in 1 2 3 4 5;
    do
        R1=$HOME/workdir/assignment/ngs1_project/Shuffled_reads/out/shuff_SRR8797509_1.part_001.part_00${SAMPLE}.fastq
        R2=$HOME/workdir/assignment/ngs1_project/Shuffled_reads/out/shuff_SRR8797509_2.part_001.part_00${SAMPLE}.fastq
        newf1=$HOME/workdir/assignment/aggr_trimmed/shuff_SRR8797509_1.part_001.part_00${SAMPLE}.pe.trim.fastq
        newf2=$HOME/workdir/assignment/aggr_trimmed/shuff_SRR8797509_2.part_001.part_00${SAMPLE}.pe.trim.fastq
        newf1U=$HOME/workdir/assignment/aggr_trimmed/shuff_SRR8797509_1.part_001.part_00${SAMPLE}.se.trim.fastq
        newf2U=$HOME/workdir/assignment/aggr_trimmed/shuff_SRR8797509_2.part_001.part_00${SAMPLE}.se.trim.fastq

        adap="/home/emmyneutron/miniconda3/envs/ngs1/share/trimmomatic-0.38-1/adapters"

        trimmomatic PE -threads 1 -phred33 -trimlog trimLogFile -summary statsSummaryFile  $R1 $R2 $newf1 $newf1U $newf2 $newf2U \
        ILLUMINACLIP:$adap/TruSeq3-PE.fa:2:30:10:1 SLIDINGWINDOW:2:30 MINLEN:36 
    done

## Data Alignment

# install the reqired tool to make alignment with BWA
#```conda install -c bioconda bwa ```
# indexing 
```mkdir ~/workdir/Assignment/bwa_align/bwaIndex && cd ~/workdir/Assignment/bwa_align/bwaIndex```
```ln -s ~/workdir/sample_data/gencode.v29.pc_transcripts.chr22.simplified.fa .```
```bwa index -a bwtsw gencode.v29.pc_transcripts.chr22.simplified.fa```

# Aligning the samples before shuffling to human chr 22 using bwa
```cd ~/workdir/Assignment/bwa_align```
for SAMPLE in 1 2 3 4 5;
    do
        R1=$HOME/workdir/Assignment/Mild_trimmed/main_SRR8797509_1.part_00${SAMPLE}.pe.trim.fastq
        R2=$HOME/workdir/Assignment/Mild_trimmed/main_SRR8797509_2.part_00${SAMPLE}.pe.trim.fastq
        /usr/bin/time -v bwa mem bwaIndex/gencode.v29.pc_transcripts.chr22.simplified.fa $R1 $R2 > SRR8797509_part_00${SAMPLE}.sam
    done

## install the reqired tool to make alignment with hisat
#```conda install -c bioconda hisat2 ```
# Indexing
mkdir -p ~/workdir/Assignment/hisat_align/hisatIndex && cd ~/workdir/Assignment/hisat_align/hisatIndex
ln -s ~/workdir/sample_data/chr22_with_ERCC92.fa .
hisat2_extract_splice_sites.py ~/workdir/sample_data/chr22_with_ERCC92.gtf > splicesites.tsv
hisat2_extract_exons.py ~/workdir/sample_data/chr22_with_ERCC92.gtf > exons.tsv
hisat2-build -p 1 --ss splicesites.tsv --exon exons.tsv chr22_with_ERCC92.fa chr22_with_ERCC92

#Aligning the samples after shuffling to human chr 22 using hisat 
```cd ~/workdir/Assignment/hisat_align```
for SAMPLE in 1 2 3 4 5;
    do
        R1=$HOME/workdir/Assignment/Aggressive_trimmed/shuff_SRR8797509_1.part_001.part_00${SAMPLE}.pe.trim.fastq
        R2=$HOME/workdir/Assignment/Aggressive_trimmed/shuff_SRR8797509_2.part_001.part_00${SAMPLE}.pe.trim.fastq
        hisat2 -p 1 -x hisatIndex/chr22_with_ERCC92 --dta --rna-strandness RF -1 $R1 -2 $R2 -S shuff_SRR8797509_part_00${SAMPLE}.sam
    done


## Data Assembly 
#Assembly for the files resulted from BWA alignment on unshuffled data (main_reads)
```cd ~/workdir/Assignment/bwa_align```

#Prepare the SAM file for assembly
#install Samtools
#conda install samtools

#convert the SAM file into BAM file 
samtools view -bS SRR8797509_part_001.sam > SRR8797509_part_001.bam
samtools view -bS SRR8797509_part_002.sam > SRR8797509_part_002.bam
samtools view -bS SRR8797509_part_003.sam > SRR8797509_part_003.bam
samtools view -bS SRR8797509_part_004.sam > SRR8797509_part_004.bam
samtools view -bS SRR8797509_part_005.sam > SRR8797509_part_005.bam

#convert the BAM file to a sorted BAM file. 
samtools sort SRR8797509_part_001.bam -o SRR8797509_part_001.sorted.bam
samtools sort SRR8797509_part_002.bam -o SRR8797509_part_002.sorted.bam
samtools sort SRR8797509_part_003.bam -o SRR8797509_part_003.sorted.bam
samtools sort SRR8797509_part_004.bam -o SRR8797509_part_004.sorted.bam
samtools sort SRR8797509_part_005.bam -o SRR8797509_part_005.sorted.bam

#Export some useful statistics report for each sample indvidually
for f in 1 2 3 4 5;
    do
        samtools flagstat SRR8797509_part_00$f.sorted.bam > useful_stat_$f.txt;
    done

#install required tool for assembly 
#install stringtie

# Assembly without known annotations
for SAMPLE in 1 2 3 4 5;
    do
        stringtie SRR8797509_part_00${SAMPLE}.sorted.bam --rf -l ref_free_${SAMPLE} -o ref_free_${SAMPLE}.gtf
    done

# Assembly with known previous annotations
for SAMPLE in 1 2 3 4 5;
    do
        stringtie SRR8797509_part_00${SAMPLE}.sorted.bam --rf -l ref_sup_${SAMPLE} -G ~/workdir/sample_data/chr22_with_ERCC92.gtf -o ref_sup_${SAMPLE}.gtf 
    done

## Assembly for the files resulted from Hisat alignment on shuffled data

```cd ~/workdir/Assignment/hisat_align```

## convert the SAM file into BAM file 
samtools view -bS shuff_SRR8797509_part_001.sam > shuff_SRR8797509_part_001.bam
samtools view -bS shuff_SRR8797509_part_002.sam > shuff_SRR8797509_part_002.bam
samtools view -bS shuff_SRR8797509_part_003.sam > shuff_SRR8797509_part_003.bam
samtools view -bS shuff_SRR8797509_part_004.sam > shuff_SRR8797509_part_004.bam
samtools view -bS shuff_SRR8797509_part_005.sam > shuff_SRR8797509_part_005.bam

## convert the BAM file to a sorted BAM file. 
samtools sort shuff_SRR8797509_part_001.bam -o shuff_SRR8797509_part_001.sorted.bam
samtools sort shuff_SRR8797509_part_002.bam -o shuff_SRR8797509_part_002.sorted.bam
samtools sort shuff_SRR8797509_part_003.bam -o shuff_SRR8797509_part_003.sorted.bam
samtools sort shuff_SRR8797509_part_004.bam -o shuff_SRR8797509_part_004.sorted.bam
samtools sort shuff_SRR8797509_part_005.bam -o shuff_SRR8797509_part_005.sorted.bam

## Export some useful statistics report for each sample indvidually
for f in 1 2 3 4 5;
    do
        samtools flagstat shuff_SRR8797509_part_00$f.sorted.bam > shuff_useful_stat_$f.txt;
    done

# install required tool for assembly 
# install stringtie

## Assembly without known annotations
for SAMPLE in 1 2 3 4 5;
    do
        stringtie shuff_SRR8797509_part_00${SAMPLE}.sorted.bam --rf -l ref_free_${SAMPLE} -o ref_free_${SAMPLE}.gtf
    done

# Assembly with known previous annotations
for SAMPLE in 1 2 3 4 5;
    do
        stringtie shuff_SRR8797509_part_00${SAMPLE}.sorted.bam --rf -l ref_sup_${SAMPLE} -G ~/workdir/sample_data/chr22_with_ERCC92.gtf -o ref_sup_${SAMPLE}.gtf 
    done

## Using GTF-Compare to Compare the Generated Annotation Files to a Reference Annotation.
conda create -n ngs-gtf python=3.6 anaconda
source activate ngs-gtf
conda install -c conda-forge pypy3.5
wget https://bootstrap.pypa.io/get-pip.py
pypy3 get-pip.py
pypy3 -m pip install gffutils numpy tqdm 'intervaltree<3.0'

mkdir -p ~/workdir/Assignment/gtf-compare/gtfs && cd ~/workdir/Assignment/gtf-compare/gtfs
ln -s ~/workdir/Assignment/bwa_align/ref_free_*.gtf .
ln -s ~/workdir/Assignment/bwa_align/ref_sup_*.gtf .
# ln -s ~/workdir/sample_data/chr22_with_ERCC92.gtf .
mkdir -p ~/workdir/Assignment/gtf-compare/method_one && cd ~/workdir/Assignment/gtf-compare/method_one
wget https://raw.githubusercontent.com/abdelrahmanMA/gtf-compare/master/code/comp.py
wget https://raw.githubusercontent.com/abdelrahmanMA/gtf-compare/master/code/stat.py

for f in 1 2 3 4 5;
        do
                pypy3 comp.py -r ../gtfs/ref_sup_*.gtf ../gtfs/ref_free_*.gtf
                pypy3 stat.py
        done

# pypy3 comp.py -r ../gtfs/ref_sup_*.gtf ../gtfs/chr22_with_ERCC92.gtf

## Differential_expression
mkdir -p ~/workdir/assignment/diff_exp && cd ~/workdir/assignment/diff_exp/
mkdir ~/workdir/assignment/ngs1_project/out && cd ~/workdir/assignment/ngs1_project/out|mv ~/workdir/assignment/ngs1_project/main_reads/out/*.fastq out| mv ~/workdir/assignment/ngs1_project/shuff_reads/out/*.fastq out
#wget -c https://0x0.st/zK57.gz -O ref.tar.gz
#tar xvzf ref.tar.gz
wget -c https://raw.githubusercontent.com/mr-eyes/nu-ngs01/master/Day-6/deseq1.r
wget -c https://raw.githubusercontent.com/mr-eyes/nu-ngs01/master/Day-6/draw-heatmap.r

#1 Setup enviornemnt
#conda activate ngs1
# conda install kallisto
# conda install samtools

# Install subread, we will use featureCount : a software program developed for counting reads to genomic features such as genes, exons, promoters and genomic bins.
conda install subread

# install r and dependicies
#conda install r
conda install -y bioconductor-deseq r-gplots
RUNLOG=runlog.txt

#Step 2 (Quantification)Step 
GTF=~/workdir/sample_data/chr22_with_ERCC92.gtf

# Generate the counts.
featureCounts -a $GTF -g gene_name -o counts.txt  ~/workdir/Assignment/bwa_align/SRR8797509*.bam  ~/workdir/Assignment/hisat_align/shuff_SRR8797509*.bam
# Simplify the file to keep only the count columns.
cat counts.txt | cut -f 1,7-12 > simple_counts.txt
# Analyze the counts with DESeq1.
cat simple_counts.txt | Rscript deseq1.r 5x5 > results_deseq1.tsv

#View only rows with pval < 0.05
cat results_deseq1.tsv | awk ' $8 < 0.05 { print $0 }' > filtered_results_deseq1.tsv
cat filtered_results_deseq1.tsv | Rscript draw-heatmap.r > hisat_output.pdf

#Results: u can view results at this link
shorturl.at/eoRW9
